
# âš¡ Activation Functions

Activation functions introduce non-linearity into neural networks, enabling them to learn complex patterns.

| Activation     | Formula                         | Derivative            | Notes                                   |
| -------------- | ------------------------------- | --------------------- | --------------------------------------- |
| **Sigmoid**    | Ïƒ(x) = 1 / (1 + eâ»Ë£)            | Ïƒ(x)(1 - Ïƒ(x))        | Squashes input to (0, 1); can saturate  |
| **Tanh**       | tanh(x) = (eË£ - eâ»Ë£)/(eË£ + eâ»Ë£) | 1 - tanhÂ²(x)          | Output in (-1, 1); zero-centered        |
| **ReLU**       | max(0, x)                       | 1 if x > 0 else 0     | Efficient; sparse activation            |
| **Leaky ReLU** | max(Î±x, x), Î± â‰ˆ 0.01            | 1 if x > 0 else Î±     | Prevents dead neurons                   |
| **Softmax**    | eË£â± / Î£eË£Ê²                      | Gradient more complex | Used in output layer for classification |

---

## ğŸ” Forward Pass

The forward pass computes the output of the network given an input.

For a layer:

```
z = WÂ·x + b
a = activation(z)
```

---

## ğŸ”™ Backward Pass (Backpropagation)

Backpropagation computes the gradient of the loss with respect to each parameter using the chain rule.

Chain Rule:

```
âˆ‚L/âˆ‚x = âˆ‚L/âˆ‚z Â· âˆ‚z/âˆ‚x
```

Backprop through a layer:

```
Given:
z = WÂ·x + b
a = activation(z)
Loss = L(a)

Then:
âˆ‚L/âˆ‚z = âˆ‚L/âˆ‚a Â· activation'(z)
âˆ‚L/âˆ‚W = âˆ‚L/âˆ‚z Â· xáµ€
âˆ‚L/âˆ‚x = Wáµ€ Â· âˆ‚L/âˆ‚z
âˆ‚L/âˆ‚b = âˆ‚L/âˆ‚z
```

---

## ğŸ”— Computational Graph

Neural networks can be seen as computational graphs (DAGs) where:

* **Nodes** = operations
* **Edges** = data (tensors) flowing between operations

### Why it's useful:

* Enables **automatic differentiation**
* Tracks all operations during the forward pass
* Enables efficient backward gradient computation

---

## ğŸŒŠ Gradient Flow Through Common Operations

| Operation                      | Gradient Behavior                        |
| ------------------------------ | ---------------------------------------- |
| **Addition:** z = x + y        | âˆ‚L/âˆ‚x = âˆ‚L/âˆ‚z, âˆ‚L/âˆ‚y = âˆ‚L/âˆ‚z             |
| **Multiplication:** z = x \* y | âˆ‚L/âˆ‚x = âˆ‚L/âˆ‚z Â· y, âˆ‚L/âˆ‚y = âˆ‚L/âˆ‚z Â· x     |
| **Max(x, y)**                  | Pass gradient to max input; other gets 0 |

---

## ğŸ§  Efficient Backpropagation

Key ideas:

1. **Cache forward values** for reuse during backward pass.
2. **Re-use gradients** when possible to avoid recomputation.

---

## ğŸ”„ General Backpropagation Algorithm

1. **Topological sort**: Visit nodes in order from input to output.
2. **Forward pass**: Compute outputs of each node.
3. **Backward pass**:

   * Start with âˆ‚L/âˆ‚output = 1
   * Use chain rule to compute gradients backward

ğŸ§  Cost: Same order of complexity as the forward pass.

---

## âš™ï¸ Partial Derivatives in Neural Networks

For a layer:

```
z = WÂ·x + b
```

* âˆ‚z/âˆ‚x = W
* âˆ‚z/âˆ‚W = xáµ€
* âˆ‚z/âˆ‚b = I (identity matrix)

---

## ğŸ”² Jacobians and Shapes

* The Jacobian of a vector-valued function is a matrix of all partial derivatives.
* For a scalar loss `S` and weight matrix `W`:

  ```
  âˆ‚S/âˆ‚W is shaped like W
  ```

---

## ğŸ§® Outer Product Gradient (for weights)

For a layer:

```
z = WÂ·x + b
```

If:

* `Î”` = âˆ‚L/âˆ‚z (upstream gradient)
* `x` is the input

Then:

```
âˆ‚L/âˆ‚W = Î”áµ€ Â· xáµ€
```

This is an **outer product** of the two vectors, resulting in a matrix of shape `W`.

---

## ğŸ¤– Automatic Differentiation (Autodiff)

Frameworks like PyTorch, TensorFlow use **autodiff** to compute gradients automatically.

How it works:

1. Each operation knows how to compute its **local gradient**.
2. The framework constructs a **computation graph** during forward pass.
3. On `.backward()`, it automatically applies the **chain rule** through the graph.

---

## âœ… Manual Gradient Checking

Used to validate your backpropagation implementation.

### Finite Difference Method:

```
f'(x) â‰ˆ (f(x + h) - f(x - h)) / (2h)
```

Steps:

1. Choose small h = 1e-4
2. Compare to backprop result
3. If the difference is < 1e-2, you're usually safe.

â›” Very slow (not used in training), but essential for debugging.

---

## âœ… Summary Table

| Concept                 | Description                               |
| ----------------------- | ----------------------------------------- |
| **Activation**          | Introduces non-linearity                  |
| **Forward Pass**        | Compute network output                    |
| **Backward Pass**       | Compute gradients using chain rule        |
| **Gradient Flow**       | Follows dependencies in computation graph |
| **Partial Derivatives** | Local derivatives needed for chain rule   |
| **Jacobian**            | Matrix of partial derivatives             |
| **AutoDiff**            | Automates gradient computation            |
| **Manual Checking**     | Finite difference to validate gradients   |


