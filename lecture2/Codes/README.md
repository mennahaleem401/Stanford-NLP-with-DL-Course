
## ğŸ§  Stanford NLP with Deep Learning - Word Embeddings (GloVe)

### ğŸ”¹ ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ GloVe

```python
import gensim.downloader as api

def load_embedding_model():
    """Load GloVe Vectors (200-dimensional)"""
    wv_from_bin = api.load("glove-wiki-gigaword-200")
    print("Loaded vocab size %i" % len(wv_from_bin.index_to_key))
    return wv_from_bin

wv_from_bin = load_embedding_model()
```

---

### ğŸ”¹ ØªÙƒÙˆÙŠÙ† Ø§Ù„Ù…ØµÙÙˆÙØ© Ù…Ù† Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©

```python
import numpy as np
import random

def get_matrix_of_vectors(wv_from_bin, required_words):
    words = list(wv_from_bin.index_to_key)
    random.seed(225)
    random.shuffle(words)

    word2ind = {}
    M = []
    curInd = 0

    for w in words:
        try:
            M.append(wv_from_bin.get_vector(w))
            word2ind[w] = curInd
            curInd += 1
        except KeyError:
            continue

    for w in required_words:
        if w not in words:
            try:
                M.append(wv_from_bin.get_vector(w))
                word2ind[w] = curInd
                curInd += 1
            except KeyError:
                continue

    M = np.stack(M)
    return M, word2ind
```

---

### ğŸ”¹ ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„Ø£Ø¨Ø¹Ø§Ø¯ Ù„Ù„ØªÙ…Ø«ÙŠÙ„ Ø§Ù„Ø¨ØµØ±ÙŠ

```python
from sklearn.decomposition import PCA

def reduce_to_k_dim(M, k=2):
    pca = PCA(n_components=k)
    M_reduced = pca.fit_transform(M)
    return M_reduced

M, word2ind = get_matrix_of_vectors(wv_from_bin, words)
M_reduced = reduce_to_k_dim(M, k=2)

M_lengths = np.linalg.norm(M_reduced, axis=1)
M_reduced_normalized = M_reduced / M_lengths[:, np.newaxis]
```

---

### ğŸ”¹ Ø­Ø³Ø§Ø¨ Cosine Similarity

```python
from numpy import dot
from numpy.linalg import norm

def cosine_similarity(vec1, vec2):
    return dot(vec1, vec2) / (norm(vec1) * norm(vec2))
```

---

### ğŸ”¹ Ø§Ù„ØªÙ…Ø«ÙŠÙ„ Ø§Ù„Ø¹ÙƒØ³ÙŠ Ù„Ù„ÙÙƒØªÙˆØ± (Vector Inversion)

```python
neg_vec = -wv_from_bin.get_vector("happy")  # for example
```

---

### ğŸ”¹ Ø­Ù„ Analogies

```python
import pprint
pprint.pprint(wv_from_bin.most_similar(positive=['woman', 'grandfather'], negative=['man']))
# Expected: grandmother
```

Ù…Ø¹Ø§Ø¯Ù„Ø© Ø§Ù„Ù€ analogy:

$$
\text{x} = \text{grandfather} - \text{man} + \text{woman}
$$

---

### ğŸ”¹ Ø£Ù…Ø«Ù„Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø®Ø·Ø§Ø¡ ÙÙŠ analogies

#### Ù…Ø«Ø§Ù„:

```python
pprint.pprint(wv_from_bin.most_similar(positive=['foot', 'glove'], negative=['hand']))
```

Ø§Ù„Ù†ØªÙŠØ¬Ø© ÙƒØ§Ù†Øª:

```python
[('45,000-square', 0.49), ('10,000-square', ...)]
```

**Ø§Ù„Ø³Ø¨Ø¨:** Ù„Ø£Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ØªØ¹Ù„Ù… Ù…Ù† Ø¨ÙŠØ§Ù†Ø§Øª ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø§Ø³ØªØ®Ø¯Ø§Ù…Ø§Øª Ø´Ø§Ø¦Ø¹Ø© Ù„ÙƒÙ„Ù…Ø© "foot" ÙƒÙˆØ­Ø¯Ø© Ù‚ÙŠØ§Ø³ ÙˆÙ„ÙŠØ³ ÙƒØ¬Ø²Ø¡ Ù…Ù† Ø§Ù„Ø¬Ø³Ù… ÙÙ‚Ø·.

---

### ğŸ”¹ ØªØ¬Ø±Ø¨Ø© Bias ÙÙŠ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬

```python
pprint.pprint(wv_from_bin.most_similar(positive=['man', 'profession'], negative=['woman']))
pprint.pprint(wv_from_bin.most_similar(positive=['woman', 'profession'], negative=['man']))
```

**Ø§Ù„Ù†ØªÙŠØ¬Ø©:**
ØºØ§Ù„Ø¨Ù‹Ø§ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…Ø±ØªØ¨Ø·Ø© Ø¨Ù€ "man" ÙÙŠÙ‡Ø§ ÙˆØ¸Ø§Ø¦Ù Ø°Ø§Øª Ù…ÙƒØ§Ù†Ø© Ø¹Ø§Ù„ÙŠØ©ØŒ Ø¨ÙŠÙ†Ù…Ø§ Ø§Ù„Ù…Ø±ØªØ¨Ø·Ø© Ø¨Ù€ "woman" ÙÙŠÙ‡Ø§ ÙˆØ¸Ø§Ø¦Ù ØªÙ‚Ù„ÙŠØ¯ÙŠØ© Ø£ÙƒØªØ±.

#### âœ”ï¸ Ø´Ø±Ø­ Ø§Ù„Ø§Ù†Ø­ÙŠØ§Ø² (Bias)

Bias ÙÙŠ word vectors Ù…Ù…ÙƒÙ† ÙŠØ­ØµÙ„ Ø¨Ø³Ø¨Ø¨:

* Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù„ÙŠ ØªÙ… Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¹Ù„ÙŠÙ‡Ø§ ÙƒØ§Ù†Øª Ù…Ù„ÙŠØ§Ù†Ø© Ø¨Ø§Ù„ØªÙ…ÙŠÙŠØ²Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ Ø§Ù„Ù…Ø¬ØªÙ…Ø¹ (Ø²ÙŠ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø¥Ø®Ø¨Ø§Ø±ÙŠØ© Ø£Ùˆ Ù…Ù‚Ø§Ù„Ø§Øª Ø§Ù„Ø¥Ù†ØªØ±Ù†Øª).

#### âœ”ï¸ Ø·Ø±ÙŠÙ‚Ø© ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„Ø§Ù†Ø­ÙŠØ§Ø²:

Ø·Ø±ÙŠÙ‚Ø© "Hard Debiasing" Ø²ÙŠ Ù…Ø§ Ø´Ø±Ø­ØªÙ‡Ø§ Bolukbasi et al. (2016):

* Ø¨ØªØ­Ø¯Ø¯ Ø§ØªØ¬Ø§Ù‡ Ø§Ù„Ù€ gender (Ù…Ø«Ù„Ø§Ù‹: he - she).
* Ø¨Ø¹Ø¯ÙŠÙ† Ø¨ØªØ­Ø§ÙˆÙ„ ØªØ²ÙŠÙ„ Ù‡Ø°Ø§ Ø§Ù„Ø§ØªØ¬Ø§Ù‡ Ù…Ù† Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù„ÙŠ Ø§Ù„Ù…ÙØ±ÙˆØ¶ ØªÙƒÙˆÙ† Ù…Ø­Ø§ÙŠØ¯Ø©.
